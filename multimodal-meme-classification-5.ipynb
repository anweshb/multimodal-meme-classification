{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os \nimport random\nimport matplotlib.pyplot as plt\nimport json\nimport numpy as np\nimport cv2\nimport typing as t\nfrom tqdm import tqdm\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.stem import SnowballStemmer\nimport tensorflow as tf\n\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import Input,Conv2D,MaxPooling2D,Flatten,Dense,Dropout, Attention,concatenate, Layer, TextVectorization, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.applications.vgg19 import preprocess_input\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import metrics\nfrom tensorflow.keras.utils import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom sklearn.metrics import classification_report","metadata":{"execution":{"iopub.status.busy":"2023-09-14T07:37:52.049321Z","iopub.execute_input":"2023-09-14T07:37:52.049709Z","iopub.status.idle":"2023-09-14T07:37:52.070779Z","shell.execute_reply.started":"2023-09-14T07:37:52.049671Z","shell.execute_reply":"2023-09-14T07:37:52.067391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir visual_attn_model_ckpts","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:54:48.415263Z","iopub.execute_input":"2023-09-14T02:54:48.416776Z","iopub.status.idle":"2023-09-14T02:54:49.428307Z","shell.execute_reply.started":"2023-09-14T02:54:48.416737Z","shell.execute_reply":"2023-09-14T02:54:49.426877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !rm -r /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:54:49.431325Z","iopub.execute_input":"2023-09-14T02:54:49.435987Z","iopub.status.idle":"2023-09-14T02:54:49.442011Z","shell.execute_reply.started":"2023-09-14T02:54:49.435930Z","shell.execute_reply":"2023-09-14T02:54:49.440984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('stopwords', quiet=True)\nnltk.download('wordnet', quiet=True)\nnltk.download('omw-1.4', quiet=True)\nnltk.download('averaged_perceptron_tagger', quiet=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:54:49.445338Z","iopub.execute_input":"2023-09-14T02:54:49.446589Z","iopub.status.idle":"2023-09-14T02:54:49.749717Z","shell.execute_reply.started":"2023-09-14T02:54:49.446551Z","shell.execute_reply":"2023-09-14T02:54:49.748718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !unzip /root/nltk_data/corpora/wordnet -d /root/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:54:49.751371Z","iopub.execute_input":"2023-09-14T02:54:49.752063Z","iopub.status.idle":"2023-09-14T02:54:49.756875Z","shell.execute_reply.started":"2023-09-14T02:54:49.752028Z","shell.execute_reply":"2023-09-14T02:54:49.755816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_dir = '/kaggle/input/multimodal-hate-speech/'\nprint(os.listdir(base_dir))","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:54:49.758598Z","iopub.execute_input":"2023-09-14T02:54:49.759425Z","iopub.status.idle":"2023-09-14T02:54:49.775275Z","shell.execute_reply.started":"2023-09-14T02:54:49.759392Z","shell.execute_reply":"2023-09-14T02:54:49.774375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"splits_path = os.path.join(base_dir, 'splits/')\nprint(os.listdir(splits_path))","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:54:49.776871Z","iopub.execute_input":"2023-09-14T02:54:49.777225Z","iopub.status.idle":"2023-09-14T02:54:49.792618Z","shell.execute_reply.started":"2023-09-14T02:54:49.777193Z","shell.execute_reply":"2023-09-14T02:54:49.791668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Function to read file IDs of train, val, test splits\ndef read_ids(path):\n    with open(path) as f:\n        lines = f.readlines()\n        lines = [x.strip() for x in lines]\n        \n        return lines\n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:54:49.793565Z","iopub.execute_input":"2023-09-14T02:54:49.793864Z","iopub.status.idle":"2023-09-14T02:54:49.800755Z","shell.execute_reply.started":"2023-09-14T02:54:49.793835Z","shell.execute_reply":"2023-09-14T02:54:49.799583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_id_path = os.path.join(splits_path, 'train_ids.txt')\nval_id_path = os.path.join(splits_path, 'val_ids.txt')\ntest_id_path = os.path.join(splits_path, 'test_ids.txt')\n\ntrain_ids = read_ids(train_id_path)\nval_ids = read_ids(val_id_path)\ntest_ids = read_ids(test_id_path)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:54:49.802546Z","iopub.execute_input":"2023-09-14T02:54:49.803031Z","iopub.status.idle":"2023-09-14T02:54:49.916653Z","shell.execute_reply.started":"2023-09-14T02:54:49.802850Z","shell.execute_reply":"2023-09-14T02:54:49.915512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Training data size: {len(train_ids)}\")\nprint(f\"Validation data size: {len(val_ids)}\")\nprint(f\"Testing data size: {len(test_ids)}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:54:49.921691Z","iopub.execute_input":"2023-09-14T02:54:49.922380Z","iopub.status.idle":"2023-09-14T02:54:49.930074Z","shell.execute_reply.started":"2023-09-14T02:54:49.922344Z","shell.execute_reply":"2023-09-14T02:54:49.928838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_dir = os.path.join(base_dir, 'img_resized')\ntext_dir = os.path.join(base_dir, 'img_txt')","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:54:49.931531Z","iopub.execute_input":"2023-09-14T02:54:49.932980Z","iopub.status.idle":"2023-09-14T02:54:49.940171Z","shell.execute_reply.started":"2023-09-14T02:54:49.932945Z","shell.execute_reply":"2023-09-14T02:54:49.938971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Check if there are any missing entries from either images or text\nprint(f\"Number of images: {len(os.listdir(img_dir))}\")\nprint(f\"Number of text files: {len(os.listdir(text_dir))}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:54:49.941777Z","iopub.execute_input":"2023-09-14T02:54:49.942154Z","iopub.status.idle":"2023-09-14T02:54:57.069092Z","shell.execute_reply.started":"2023-09-14T02:54:49.942098Z","shell.execute_reply":"2023-09-14T02:54:57.067872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Checking the images which dont have corresponding text files\nremove_extension = lambda x : x.split('.')[0]\n\nmissing_text = set(map(remove_extension, os.listdir(img_dir))) - set(map(remove_extension, os.listdir(text_dir))) ","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:54:57.070981Z","iopub.execute_input":"2023-09-14T02:54:57.071390Z","iopub.status.idle":"2023-09-14T02:54:57.308230Z","shell.execute_reply.started":"2023-09-14T02:54:57.071351Z","shell.execute_reply":"2023-09-14T02:54:57.307082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_text = list(missing_text)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:54:57.309603Z","iopub.execute_input":"2023-09-14T02:54:57.309980Z","iopub.status.idle":"2023-09-14T02:54:57.320536Z","shell.execute_reply.started":"2023-09-14T02:54:57.309947Z","shell.execute_reply":"2023-09-14T02:54:57.319587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Plotting an image randomly from those which dont have corresponding image texts\n## Cell can be run multiple times to verify that these images don't have text\nrandom_image = plt.imread(os.path.join(img_dir, random.choice(missing_text))+'.jpg')\nplt.imshow(random_image)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:54:57.322004Z","iopub.execute_input":"2023-09-14T02:54:57.322776Z","iopub.status.idle":"2023-09-14T02:54:57.787414Z","shell.execute_reply.started":"2023-09-14T02:54:57.322737Z","shell.execute_reply":"2023-09-14T02:54:57.786487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## POSSIBLY NSFW CELL BELOW","metadata":{}},{"cell_type":"code","source":"with_text_imgs = set(map(remove_extension, os.listdir(img_dir))).intersection(set(map(remove_extension, os.listdir(text_dir))))\nwith_text_imgs = list(with_text_imgs)\n\nimg_name = random.choice(with_text_imgs)\nimg_with_text_path = plt.imread(os.path.join(img_dir, img_name + '.jpg'))\nplt.imshow(img_with_text_path)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:54:57.788441Z","iopub.execute_input":"2023-09-14T02:54:57.788761Z","iopub.status.idle":"2023-09-14T02:54:58.453791Z","shell.execute_reply.started":"2023-09-14T02:54:57.788731Z","shell.execute_reply":"2023-09-14T02:54:58.452833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(os.path.join(text_dir, img_name + '.json'))","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:54:58.455163Z","iopub.execute_input":"2023-09-14T02:54:58.456155Z","iopub.status.idle":"2023-09-14T02:54:58.461584Z","shell.execute_reply.started":"2023-09-14T02:54:58.456120Z","shell.execute_reply":"2023-09-14T02:54:58.460763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(os.path.join(text_dir, img_name + '.json')) as f:\n    d = json.load(f)\n    print(d)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:54:58.463100Z","iopub.execute_input":"2023-09-14T02:54:58.464125Z","iopub.status.idle":"2023-09-14T02:54:58.480965Z","shell.execute_reply.started":"2023-09-14T02:54:58.464093Z","shell.execute_reply":"2023-09-14T02:54:58.479984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(text: str) -> str:\n    \n    # Convert to lowercase\n    text = text.lower()\n\n    # Non-word character Removal\n    text = text.replace('[^\\w\\s]', '')\n    \n    # Digits Removal\n    text = text.replace('\\d', '')\n\n    \n    # Remove punctuation\n    PUNCT_TO_REMOVE = string.punctuation\n    text = text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n    \n    \n    # Remove stopwords\n    STOPWORDS = set(stopwords.words('english'))\n    text = \" \".join([word for word in text.split() if word not in STOPWORDS])\n    \n    \n    # Stem words\n    stemmer = SnowballStemmer(language='english')\n#     wordnet_map = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"J\": wordnet.ADJ, \"R\": wordnet.ADV}\n#     pos_tagged_text = nltk.pos_tag(text.split())\n    text = \" \".join([stemmer.stem(word) for word in text.split()])\n    \n\n    # Remove URLs\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    text = url_pattern.sub(r'', text)\n\n    # Remove HTML tags\n    html_pattern = re.compile('<.*?>')\n    text = html_pattern.sub(r'', text)\n\n    return text\n\ndef preprocess_text_from_path(path : str) -> str:\n    \n    try:\n        with open(path) as f:\n            d = json.load(f)\n            text = d['img_text']\n        \n    except Exception as e:\n        text = ''\n    \n#     return text\n    return preprocess_text(text)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:54:58.482499Z","iopub.execute_input":"2023-09-14T02:54:58.483065Z","iopub.status.idle":"2023-09-14T02:54:58.494449Z","shell.execute_reply.started":"2023-09-14T02:54:58.483032Z","shell.execute_reply":"2023-09-14T02:54:58.493577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testext1='Hey there this is a test to see if this text preprocessor is working as intended'\npreprocess_text(testext1)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:54:58.495880Z","iopub.execute_input":"2023-09-14T02:54:58.496286Z","iopub.status.idle":"2023-09-14T02:54:58.511540Z","shell.execute_reply.started":"2023-09-14T02:54:58.496256Z","shell.execute_reply":"2023-09-14T02:54:58.510479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_path = '/kaggle/input/multimodal-hate-speech/MMHS150K_GT.json'","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:54:58.513568Z","iopub.execute_input":"2023-09-14T02:54:58.514015Z","iopub.status.idle":"2023-09-14T02:54:58.518655Z","shell.execute_reply.started":"2023-09-14T02:54:58.513982Z","shell.execute_reply":"2023-09-14T02:54:58.517682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_for_tokenizer(ids):\n    all_texts = []\n    \n    \n    for img_id in ids:\n        \n        try:\n            txt_file_name = os.path.join(text_dir, img_id + '.json')\n\n            with open(txt_file_name) as f:\n                txt_dict = json.load(f)\n            \n            txt = txt_dict['img_text']\n            processed_txt = preprocess_text(txt)\n            all_texts.append(processed_txt)\n            \n        except Exception:\n            txt = ''\n            all_texts.append(txt)\n            \n            \n    return all_texts","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:54:58.520342Z","iopub.execute_input":"2023-09-14T02:54:58.521058Z","iopub.status.idle":"2023-09-14T02:54:58.528817Z","shell.execute_reply.started":"2023-09-14T02:54:58.521026Z","shell.execute_reply":"2023-09-14T02:54:58.527853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"txt = text_for_tokenizer(train_ids)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:54:58.529869Z","iopub.execute_input":"2023-09-14T02:54:58.530405Z","iopub.status.idle":"2023-09-14T03:01:42.577630Z","shell.execute_reply.started":"2023-09-14T02:54:58.530373Z","shell.execute_reply":"2023-09-14T03:01:42.576586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_texts = txt","metadata":{"execution":{"iopub.status.busy":"2023-09-14T03:01:42.578947Z","iopub.execute_input":"2023-09-14T03:01:42.579284Z","iopub.status.idle":"2023-09-14T03:01:42.587429Z","shell.execute_reply.started":"2023-09-14T03:01:42.579251Z","shell.execute_reply":"2023-09-14T03:01:42.586496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_features = 50000  #max number of tokens...the max number of unique words from our text data\nsequence_length = 50 #output length..if some are not of this length they get padded\n\nvectorization_layer = TextVectorization(\n    max_tokens=max_features,\n    output_mode='int',\n    output_sequence_length=sequence_length,\n    pad_to_max_tokens=True)\n\nvectorization_layer.adapt(training_texts)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T03:01:42.590560Z","iopub.execute_input":"2023-09-14T03:01:42.591159Z","iopub.status.idle":"2023-09-14T03:01:56.548281Z","shell.execute_reply.started":"2023-09-14T03:01:42.591133Z","shell.execute_reply":"2023-09-14T03:01:56.547250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip glove.6B.zip","metadata":{"execution":{"iopub.status.busy":"2023-09-14T03:01:56.549949Z","iopub.execute_input":"2023-09-14T03:01:56.550293Z","iopub.status.idle":"2023-09-14T03:05:01.048841Z","shell.execute_reply.started":"2023-09-14T03:01:56.550259Z","shell.execute_reply":"2023-09-14T03:05:01.047612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_index = {}\nf = open(os.path.join('./', 'glove.6B.300d.txt'))\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\n\nembedding_dim = 300\n\n# Define the vocabulary size (should match your TextVectorization max_tokens)\nvocab_size = len(vectorization_layer.get_vocabulary())\n\n# Create an embedding matrix with zeros\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\n\nfor i, word in enumerate(vectorization_layer.get_vocabulary()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2023-09-14T03:05:01.050555Z","iopub.execute_input":"2023-09-14T03:05:01.050856Z","iopub.status.idle":"2023-09-14T03:05:34.437909Z","shell.execute_reply.started":"2023-09-14T03:05:01.050827Z","shell.execute_reply":"2023-09-14T03:05:34.436903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def binarize_labels(labels):\n    \n    binarized_labels = []\n    \n    for label_lst in labels:\n        processed_lst = [1 if x != 0 and x != 1 else x for x in label_lst]\n    \n        # Count the number of 0s and 1s in the processed list\n        count_0 = processed_lst.count(0)\n        count_1 = processed_lst.count(1)\n    \n        # Return 0 if there are more 0s, otherwise return 1\n        if count_0 > count_1:\n            binarized_labels.append(0)\n        else:\n            binarized_labels.append(1)\n    \n    return binarized_labels","metadata":{"execution":{"iopub.status.busy":"2023-09-14T03:05:34.444673Z","iopub.execute_input":"2023-09-14T03:05:34.444993Z","iopub.status.idle":"2023-09-14T03:05:34.451892Z","shell.execute_reply.started":"2023-09-14T03:05:34.444965Z","shell.execute_reply":"2023-09-14T03:05:34.450890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_image(image_path):\n    img = plt.imread(image_path)\n    img = cv2.resize(img, (224,224))\n    img = preprocess_input(img)\n    img = img/255.0\n    return img\n\n\ndef load_data(ids, batch_size = 32, mode = 'both', shuffle = False):\n    \n    image_paths = []\n    text_paths = []\n    labels = []\n    \n\n    for data_id in tqdm(ids):\n            img_path = os.path.join(img_dir, data_id + '.jpg')\n            text_path = os.path.join(text_dir, data_id + '.json')\n            \n            image_paths.append(img_path)\n            text_paths.append(text_path)\n    \n    with open(labels_path) as f:\n        label_dict = json.load(f)\n        \n    for data_id in ids:\n#         print(data_id)\n#         print(len(label_dict[data_id]['labels']))\n        labels.append(label_dict[data_id]['labels'])\n    \n    num_samples = len(image_paths)\n    indices = np.arange(num_samples)\n    \n    print(len(labels))\n    \n    if shuffle:\n        np.random.shuffle(indices)\n\n    while True:\n        for start_idx in range(0, num_samples, batch_size):\n            end_idx = min(start_idx + batch_size, num_samples)\n            batch_indices = np.array(indices[start_idx:end_idx], dtype = 'uint8')    \n                        \n            batch_images = [preprocess_image(image_paths[i]) for i in batch_indices]\n            batch_images = np.array(batch_images)\n\n            batch_texts = [preprocess_text_from_path(text_paths[i]) for i in batch_indices]\n            batch_texts = vectorization_layer((np.array(batch_texts)))\n\n            batch_labels = binarize_labels(np.array((labels), dtype = 'object')[batch_indices])\n            \n            if mode == 'both':\n                yield (batch_images, batch_texts), np.array(batch_labels)\n            elif mode == 'img':\n                yield batch_images, np.array(batch_labels)\n            elif mode == 'text':\n                yield  batch_texts, np.array(batch_labels)\n            else:\n                raise Exception(\"Wrong mode specified\")","metadata":{"execution":{"iopub.status.busy":"2023-09-14T03:05:34.455632Z","iopub.execute_input":"2023-09-14T03:05:34.456111Z","iopub.status.idle":"2023-09-14T03:05:34.470865Z","shell.execute_reply.started":"2023-09-14T03:05:34.456079Z","shell.execute_reply":"2023-09-14T03:05:34.469866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_generator = load_data(train_ids)\n\n# To retrieve a batch of data:\nbatch_data, batch_labels = next(train_data_generator)\nbatch_images, batch_texts = batch_data","metadata":{"execution":{"iopub.status.busy":"2023-09-14T03:05:34.472501Z","iopub.execute_input":"2023-09-14T03:05:34.472857Z","iopub.status.idle":"2023-09-14T03:05:37.762388Z","shell.execute_reply.started":"2023-09-14T03:05:34.472825Z","shell.execute_reply":"2023-09-14T03:05:37.761396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_feature_extractor(): \n    input_txt = Input(shape=(50,), dtype='int64')\n    # print(input_txt\n    txt = layers.Embedding(50000,\n                            300,\n                            weights=[embedding_matrix],\n                            input_length=300,\n                            trainable=False)(input_txt)\n    text_lstm = layers.Bidirectional(layers.LSTM(256,return_sequences=True))(txt)\n    text_lstm = layers.Bidirectional(layers.LSTM(128,return_sequences=True))(text_lstm)\n    text_lstm = layers.Bidirectional(layers.LSTM(64,return_sequences=False))(text_lstm)\n    attention_layer = Attention(use_scale = True)([text_lstm, text_lstm])\n    text_flatten = Flatten()(attention_layer)\n    fc_layer = Dense(512, activation='relu')(text_flatten)\n    fc_layer = Dropout(0.3)(fc_layer)\n    fc_layer = Dense(128, activation = 'relu')(fc_layer)\n    fc_layer = Dropout(0.3)(fc_layer)\n    out_layer = Dense(64, activation = 'relu')(fc_layer)\n    # merged = concatenate([text_lstm,flatten], axis=1)\n    \n    \n#     return text_layer \n    return Model(inputs = input_txt, outputs = out_layer)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T03:05:37.763682Z","iopub.execute_input":"2023-09-14T03:05:37.764047Z","iopub.status.idle":"2023-09-14T03:05:37.773502Z","shell.execute_reply.started":"2023-09-14T03:05:37.764007Z","shell.execute_reply":"2023-09-14T03:05:37.772443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_image_feature_extractor(input_shape=(224, 224, 3)):\n    \n    # Define the input layer\n    input_tensor = Input(shape=input_shape)\n    \n    conv = Conv2D(32, 2, activation = 'relu')(input_tensor)\n    bn = BatchNormalization()(conv)\n    mp = MaxPooling2D((2,2))(bn)\n    \n    conv = Conv2D(32, 2, activation = 'relu')(mp)\n    bn = BatchNormalization()(conv)\n    mp = MaxPooling2D((2,2))(bn)\n\n    visual_attention = Attention(use_scale = True)([mp, mp])\n    \n    visual_attention = Flatten()(visual_attention)\n    \n    op = Dense(64)(visual_attention)\n    \n    feature_extractor = Model(inputs = input_tensor, outputs = op, name = 'image_feature_extractor')\n    \n    return feature_extractor","metadata":{"execution":{"iopub.status.busy":"2023-09-14T03:05:37.774899Z","iopub.execute_input":"2023-09-14T03:05:37.775558Z","iopub.status.idle":"2023-09-14T03:05:37.784814Z","shell.execute_reply.started":"2023-09-14T03:05:37.775523Z","shell.execute_reply":"2023-09-14T03:05:37.783820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_attn = text_feature_extractor()\ntext_attn.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T03:05:37.786263Z","iopub.execute_input":"2023-09-14T03:05:37.786821Z","iopub.status.idle":"2023-09-14T03:05:39.920040Z","shell.execute_reply.started":"2023-09-14T03:05:37.786783Z","shell.execute_reply":"2023-09-14T03:05:39.919289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_attention = create_image_feature_extractor()\ncnn_attention.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T03:05:39.921108Z","iopub.execute_input":"2023-09-14T03:05:39.921644Z","iopub.status.idle":"2023-09-14T03:05:40.055445Z","shell.execute_reply.started":"2023-09-14T03:05:39.921616Z","shell.execute_reply":"2023-09-14T03:05:40.050208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visual_attention_model():\n    model = Sequential()\n    \n    img_fe = create_image_feature_extractor()\n    \n    model.add(img_fe)\n    model.add(Dense(1, activation = 'sigmoid'))\n    \n    return model\n\ndef multimodal_attention_model():\n    input_image = Input(shape=(224, 224, 3))\n    input_text = Input(shape = (50), dtype='int64')  \n\n    img_fe = create_image_feature_extractor()\n    text_fe = text_feature_extractor()\n\n    image_features = img_fe(input_image)\n    text_features = text_fe(input_text)\n    \n    merged_features = concatenate([image_features, text_features])\n\n    output = Dense(1, activation='sigmoid')(merged_features)\n\n    multimodal_model = Model(inputs=[input_image, input_text], outputs=output)\n\n    return multimodal_model\n\ndef semantic_attention_model():\n    \n    model = Sequential()\n    \n    text_fe = text_feature_extractor()\n    \n    model.add(text_fe)\n    model.add(Dense(1, activation = 'sigmoid'))\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-09-14T08:10:33.526991Z","iopub.execute_input":"2023-09-14T08:10:33.527374Z","iopub.status.idle":"2023-09-14T08:10:33.537363Z","shell.execute_reply.started":"2023-09-14T08:10:33.527344Z","shell.execute_reply":"2023-09-14T08:10:33.536262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visual_attention_model = visual_attention_model()\n\nes = EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True)\n\nckpt = ModelCheckpoint(\n    filepath='/kaggle/working/visual_attn_model_ckpts/best_model.h5',  # Filepath to save the model weights.\n    monitor='val_loss',  \n    save_best_only=True, \n    save_weights_only=True,  \n    mode='min',  \n    verbose=1  \n)\nvisual_attention_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T08:10:34.831341Z","iopub.execute_input":"2023-09-14T08:10:34.831694Z","iopub.status.idle":"2023-09-14T08:10:34.958606Z","shell.execute_reply.started":"2023-09-14T08:10:34.831662Z","shell.execute_reply":"2023-09-14T08:10:34.957659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visual_attention_model.load_weights('/kaggle/working/visual_attn_model_ckpts/best_model.h5')","metadata":{"execution":{"iopub.status.busy":"2023-09-14T07:33:14.662124Z","iopub.execute_input":"2023-09-14T07:33:14.663014Z","iopub.status.idle":"2023-09-14T07:33:14.705610Z","shell.execute_reply.started":"2023-09-14T07:33:14.662972Z","shell.execute_reply":"2023-09-14T07:33:14.704595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 50\n\nsteps_per_epoch = len(train_ids)//32\nval_steps = len(val_ids)//32\n\n# Training data loader\ntrain_data_loader = load_data(train_ids, mode = 'img')  # Replace with your data loader function\n\n# Validation data loader\nval_data_loader = load_data(val_ids, mode = 'img')  # Replace with your validation data loader function\n\n#Test Data Loader\n\ntest_data_loader = load_data(test_ids, mode = 'img')\n\n\n# # Train the model using model.fit\n# history = visual_attention_model.fit(\n#     train_data_loader,\n#     epochs=epochs,\n#     steps_per_epoch=steps_per_epoch,\n#     validation_data=val_data_loader,\n#     validation_steps=val_steps,\n#     callbacks=[es, ckpt]\n# )\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T03:05:40.314814Z","iopub.execute_input":"2023-09-14T03:05:40.315497Z","iopub.status.idle":"2023-09-14T04:52:51.238817Z","shell.execute_reply.started":"2023-09-14T03:05:40.315424Z","shell.execute_reply":"2023-09-14T04:52:51.236994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visual_attention_model.evaluate(val_data_loader, steps = len(val_ids)//32)\n# print(len(val_ids))","metadata":{"execution":{"iopub.status.busy":"2023-09-14T04:53:11.882779Z","iopub.execute_input":"2023-09-14T04:53:11.883475Z","iopub.status.idle":"2023-09-14T04:53:42.632755Z","shell.execute_reply.started":"2023-09-14T04:53:11.883441Z","shell.execute_reply":"2023-09-14T04:53:42.631690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visual_attention_model.evaluate(test_data_loader, steps = len(test_ids)//32)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T04:53:42.634900Z","iopub.execute_input":"2023-09-14T04:53:42.635619Z","iopub.status.idle":"2023-09-14T04:54:51.296006Z","shell.execute_reply.started":"2023-09-14T04:53:42.635582Z","shell.execute_reply":"2023-09-14T04:54:51.294974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !mkdir semantic_attn_model_ckpts\n\nsemantic_attention_model = semantic_attention_model()\n\nes = EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True)\n\nckpt = ModelCheckpoint(\n    filepath='/kaggle/working/semantic_attn_model_ckpts/best_model.h5',  # Filepath to save the model weights.\n    monitor='val_loss',  \n    save_best_only=True, \n    save_weights_only=True,  \n    mode='min',  \n    verbose=1  \n)\nsemantic_attention_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T08:10:53.268820Z","iopub.execute_input":"2023-09-14T08:10:53.269514Z","iopub.status.idle":"2023-09-14T08:10:56.501710Z","shell.execute_reply.started":"2023-09-14T08:10:53.269481Z","shell.execute_reply":"2023-09-14T08:10:56.500698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# epochs = 50\n\nsteps_per_epoch = len(train_ids)//32\nval_steps = len(val_ids)//32\n\n# #Training data loader\n# train_data_loader = load_data(train_ids, mode = 'text')  # Replace with your data loader function\n\n# # Validation data loader\n# val_data_loader = load_data(val_ids, mode = 'text')  # Replace with your validation data loader function\n\n# test_data_loader = load_data(test_ids, mode = 'text')\n\nsemantic_attention_model.load_weights('/kaggle/input/lstm-modelh5/best_model.h5')\n\n# # Train the model using model.fit\n# history = semantic_attention_model.fit(\n#     train_data_loader,\n#     epochs=epochs,\n#     steps_per_epoch=steps_per_epoch,\n#     validation_data=val_data_loader,\n#     validation_steps=val_steps,\n#     callbacks=[es, ckpt]\n# )\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T08:11:15.673604Z","iopub.execute_input":"2023-09-14T08:11:15.673987Z","iopub.status.idle":"2023-09-14T08:11:16.261123Z","shell.execute_reply.started":"2023-09-14T08:11:15.673951Z","shell.execute_reply":"2023-09-14T08:11:16.260006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"semantic_attention_model.evaluate(val_data_loader, steps = len(val_ids)//32)\n# print(len(val_ids))","metadata":{"execution":{"iopub.status.busy":"2023-09-14T08:07:13.565548Z","iopub.execute_input":"2023-09-14T08:07:13.565998Z","iopub.status.idle":"2023-09-14T08:07:59.435735Z","shell.execute_reply.started":"2023-09-14T08:07:13.565945Z","shell.execute_reply":"2023-09-14T08:07:59.434531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"semantic_attention_model.evaluate(test_data_loader, steps = len(test_ids)//32)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T08:07:59.438235Z","iopub.execute_input":"2023-09-14T08:07:59.438624Z","iopub.status.idle":"2023-09-14T08:09:22.648285Z","shell.execute_reply.started":"2023-09-14T08:07:59.438587Z","shell.execute_reply":"2023-09-14T08:09:22.647008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !mkdir multimodal_attn_model_ckpts\n\nmultimodal_attention_model = multimodal_attention_model()\n\nes = EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True)\n\nckpt = ModelCheckpoint(\n    filepath='/kaggle/working/multimodal_attn_model_ckpts/best_mm_model.h5',  # Filepath to save the model weights.\n    monitor='val_loss',  \n    save_best_only=True, \n    save_weights_only=True,  \n    mode='min',  \n    verbose=1  \n)\nmultimodal_attention_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T08:10:59.010968Z","iopub.execute_input":"2023-09-14T08:10:59.011345Z","iopub.status.idle":"2023-09-14T08:11:02.707366Z","shell.execute_reply.started":"2023-09-14T08:10:59.011314Z","shell.execute_reply":"2023-09-14T08:11:02.706337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 50\n\nsteps_per_epoch = len(train_ids)//32\nval_steps = len(val_ids)//32\n\n# Training data loader\ntrain_data_loader = load_data(train_ids, mode = 'both')  # Replace with your data loader function\n\n# Validation data loader\nval_data_loader = load_data(val_ids, mode = 'both')  # Replace with your validation data loader function\n\ntest_data_loader = load_data(test_ids, mode = 'both')\n\n# # Train the model using model.fit\n# history = multimodal_attention_model.fit(\n#     train_data_loader,\n#     epochs=epochs,\n#     steps_per_epoch=steps_per_epoch,\n#     validation_data=val_data_loader,\n#     validation_steps=val_steps,\n#     callbacks=[es, ckpt]\n# )\n\nmultimodal_attention_model.load_weights('/kaggle/working/multimodal_attn_model_ckpts/best_mm_model.h5')","metadata":{"execution":{"iopub.status.busy":"2023-09-14T08:11:02.712500Z","iopub.execute_input":"2023-09-14T08:11:02.714916Z","iopub.status.idle":"2023-09-14T08:11:03.751965Z","shell.execute_reply.started":"2023-09-14T08:11:02.714879Z","shell.execute_reply":"2023-09-14T08:11:03.750975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multimodal_attention_model.evaluate(val_data_loader, steps = len(val_ids)//32)\n# print(len(val_ids))","metadata":{"execution":{"iopub.status.busy":"2023-09-14T06:55:51.886772Z","iopub.execute_input":"2023-09-14T06:55:51.887490Z","iopub.status.idle":"2023-09-14T06:56:35.975937Z","shell.execute_reply.started":"2023-09-14T06:55:51.887444Z","shell.execute_reply":"2023-09-14T06:56:35.974894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multimodal_attention_model.evaluate(test_data_loader, steps = len(test_ids)//32)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T06:56:35.978064Z","iopub.execute_input":"2023-09-14T06:56:35.978444Z","iopub.status.idle":"2023-09-14T06:57:38.576032Z","shell.execute_reply.started":"2023-09-14T06:56:35.978408Z","shell.execute_reply":"2023-09-14T06:57:38.574996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = multimodal_attention_model.predict(test_data_loader, steps = 1)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T07:23:37.427324Z","iopub.execute_input":"2023-09-14T07:23:37.427719Z","iopub.status.idle":"2023-09-14T07:23:37.972911Z","shell.execute_reply.started":"2023-09-14T07:23:37.427685Z","shell.execute_reply":"2023-09-14T07:23:37.971914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine_predictions(predictions_list):\n    \"\"\"\n    Combine binary predictions from multiple models using majority voting.\n\n    Args:\n        predictions_list (list): List of binary predictions from multiple models,\n                                 each of shape (N, 1).\n\n    Returns:\n        list: List of combined predictions, where each element is either 0 or 1.\n    \"\"\"\n    num_samples = len(predictions_list[0])  # Assuming all models have the same number of samples\n    combined_predictions = []\n\n    for i in range(num_samples):\n        # Extract predictions for the i-th sample from all models\n        sample_predictions = [model_predictions[i] for model_predictions in predictions_list]\n\n        # Count the number of 1s and 0s\n        num_ones = np.sum(sample_predictions)\n        num_zeros = len(sample_predictions) - num_ones\n\n        # Determine the majority vote\n        if num_ones > num_zeros:\n            combined_predictions.append(1)\n        else:\n            combined_predictions.append(0)\n\n    return combined_predictions\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T07:51:51.907768Z","iopub.execute_input":"2023-09-14T07:51:51.908948Z","iopub.status.idle":"2023-09-14T07:51:51.917352Z","shell.execute_reply.started":"2023-09-14T07:51:51.908888Z","shell.execute_reply":"2023-09-14T07:51:51.915987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comb_preds = []\ntrue_preds = []\n\nfor num_steps in range(len(test_ids)//32):\n    \n    ##Dataloader being used here must have been instantiated with the 'both' argument\n    \n    x_batch, y_batch = next(test_data_loader)\n    \n    vis_preds = visual_attention_model.predict(x_batch[0])\n    sem_preds = semantic_attention_model.predict(x_batch[1])\n    mm_preds = multimodal_attention_model.predict(x_batch)\n    \n    final_preds = combine_predictions([vis_preds, sem_preds, mm_preds])\n    \n    comb_preds.extend(final_preds)\n    true_preds.extend(list(y_batch))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T08:11:20.968918Z","iopub.execute_input":"2023-09-14T08:11:20.969332Z","iopub.status.idle":"2023-09-14T08:13:45.488672Z","shell.execute_reply.started":"2023-09-14T08:11:20.969301Z","shell.execute_reply":"2023-09-14T08:13:45.487648Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(true_preds, comb_preds))","metadata":{"execution":{"iopub.status.busy":"2023-09-14T08:13:45.492316Z","iopub.execute_input":"2023-09-14T08:13:45.492640Z","iopub.status.idle":"2023-09-14T08:13:45.523124Z","shell.execute_reply.started":"2023-09-14T08:13:45.492612Z","shell.execute_reply":"2023-09-14T08:13:45.521907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for num_steps in range(len(val_ids)//32):\n    \n    ##Dataloader being used here must have been instantiated with the 'both' argument\n    \n    x_batch, y_batch = next(val_data_loader)\n    \n    vis_preds = visual_attention_model.predict(x_batch[0])\n    sem_preds = semantic_attention_model.predict(x_batch[1])\n    mm_preds = multimodal_attention_model.predict(x_batch)\n    \n    final_preds = combine_predictions([vis_preds, sem_preds, mm_preds])\n    \n    comb_preds.extend(final_preds)\n    true_preds.extend(list(y_batch))\n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-14T08:16:57.996077Z","iopub.execute_input":"2023-09-14T08:16:57.996459Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(true_preds, comb_preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}